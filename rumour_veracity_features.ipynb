{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "limiting-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-conference",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "deluxe-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(path):\n",
    "    f = open(path)\n",
    "    json_content = json.load(f)\n",
    "    f.close()\n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "boring-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_directory = 'datasets/rumoureval-2019-training-data/twitter-english'\n",
    "test_dataset_directory = 'datasets/rumoureval-2019-test-data/twitter-en-test-data'\n",
    "\n",
    "training_dataset_reddit_directory = 'datasets/rumoureval-2019-training-data/reddit-training-data'\n",
    "test_dataset_reddit_directory = 'datasets/rumoureval-2019-test-data/reddit-test-data'\n",
    "\n",
    "training_labels_json = 'datasets/rumoureval-2019-training-data/train-key.json'\n",
    "training_labels_json_2 = 'datasets/rumoureval-2019-training-data/dev-key.json'\n",
    "test_labels_json = 'datasets/final-eval-key.json'\n",
    "\n",
    "training_labels_dict = read_json_file(training_labels_json)['subtaskaenglish']\n",
    "training_labels_dict.update(read_json_file(training_labels_json_2)['subtaskaenglish'])\n",
    "test_labels_dict = read_json_file(test_labels_json)['subtaskaenglish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "basic-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, post_content, post_id, parent_post_id=None, external_urls_count=0):\n",
    "        self.post_content = post_content\n",
    "        self.post_id = post_id\n",
    "        self.category = None\n",
    "        self.parent_post_id = parent_post_id\n",
    "        self.external_urls = external_urls_count > 0\n",
    "        self.user_metadata = None\n",
    "        \n",
    "    def add_category(self, category):\n",
    "        self.category = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "changed-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceTweet:\n",
    "    def __init__(self, tweet: Tweet):\n",
    "        self.tweet = tweet\n",
    "        self.replies = []\n",
    "        \n",
    "    def add_reply(self, reply: Tweet):\n",
    "        self.replies.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "covered-aggregate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_tweets_dataset(dataset_dir_path, labels_dict):\n",
    "    topic_directories = [f.path for f in os.scandir(dataset_dir_path) if f.is_dir()]\n",
    "    topic_to_tweets_map = {}  # {topic_name: [SourceTweet, ...]}\n",
    "\n",
    "    for topic_dir in topic_directories:\n",
    "        topic_name = topic_dir.split('\\\\')[1]\n",
    "        source_tweets = []\n",
    "        \n",
    "        tweets_paths = [f.path for f in os.scandir(topic_dir) if f.is_dir()]\n",
    "        for tweet_dir in tweets_paths:\n",
    "            source_tweet_path = [f.path for f in os.scandir(tweet_dir + '/source-tweet')][0]\n",
    "            source_tweet_json = read_json_file(source_tweet_path)\n",
    "            \n",
    "            tweet = Tweet(source_tweet_json['text'], source_tweet_json['id'],\n",
    "                              source_tweet_json['in_reply_to_status_id'],\n",
    "                              len(source_tweet_json['entities']['urls']))\n",
    "            \n",
    "            source_tweet = SourceTweet(tweet)\n",
    "            source_tweets.append(source_tweet)\n",
    "            tweet.add_category(\"support\")\n",
    "            source_tweet.add_reply(tweet)\n",
    "\n",
    "            \n",
    "            reply_tweets_paths = [f.path for f in os.scandir(tweet_dir + '/replies')]\n",
    "            for reply_tweet_path in reply_tweets_paths:\n",
    "                reply_tweet_json = read_json_file(reply_tweet_path)\n",
    "                \n",
    "                reply_tweet = Tweet(reply_tweet_json['text'], reply_tweet_json['id'],\n",
    "                                        source_tweet.tweet.post_id, len(reply_tweet_json['entities']['urls']))\n",
    "                reply_tweet.add_category(labels_dict[str(reply_tweet_json['id'])])\n",
    "                source_tweet.add_reply(reply_tweet)\n",
    "        \n",
    "        topic_to_tweets_map[topic_name] = source_tweets\n",
    "        \n",
    "    return topic_to_tweets_map\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "individual-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reddit_dataset(dataset_dir_path, labels_dict):\n",
    "    topic_directories = [f.path for f in os.scandir(dataset_dir_path) if f.is_dir()]\n",
    "    topic_to_tweets_map = {}  # {topic_name: [SourceTweet, ...]}\n",
    "\n",
    "    for topic_dir in topic_directories:\n",
    "        topic_name = topic_dir.split('\\\\')[1]\n",
    "        source_tweets = []\n",
    "        \n",
    "        source_tweet_path = [f.path for f in os.scandir(topic_dir + '/source-tweet')][0]\n",
    "        source_tweet_json = read_json_file(source_tweet_path)\n",
    "\n",
    "        content = source_tweet_json['data']['children'][0]['data']['title'] + ' ' + source_tweet_json['data']['children'][0]['data']['selftext']\n",
    "        tweet = Tweet(content, source_tweet_json['data']['children'][0]['data']['id'], None, content.count(\"http\"))\n",
    "\n",
    "        source_tweet = SourceTweet(tweet)\n",
    "        source_tweets.append(source_tweet)\n",
    "        tweet.add_category(\"support\")\n",
    "        source_tweet.add_reply(tweet)\n",
    "\n",
    "        reply_tweets_paths = [f.path for f in os.scandir(topic_dir + '/replies')]\n",
    "        for reply_tweet_path in reply_tweets_paths:\n",
    "            reply_tweet_json = read_json_file(reply_tweet_path)\n",
    "            \n",
    "            if 'body' in reply_tweet_json['data']:\n",
    "                reply_tweet = Tweet(reply_tweet_json['data']['body'], reply_tweet_json['data']['id'],\n",
    "                                        source_tweet.tweet.post_id, reply_tweet_json['data']['body'].count('http'))\n",
    "                reply_tweet.add_category(labels_dict[str(reply_tweet.post_id)])\n",
    "                source_tweet.add_reply(reply_tweet)\n",
    "                \n",
    "        topic_to_tweets_map[topic_name] = source_tweets\n",
    "        \n",
    "    return topic_to_tweets_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "driven-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "training_topic_to_tweets_map = read_tweets_dataset(training_dataset_directory, training_labels_dict)\n",
    "test_topic_to_tweets_map = read_tweets_dataset(test_dataset_directory, test_labels_dict)\n",
    "\n",
    "# Reddit\n",
    "training_topic_to_reddit_map = read_reddit_dataset(training_dataset_reddit_directory, training_labels_dict)\n",
    "test_topic_to_reddit_map = read_reddit_dataset(test_dataset_reddit_directory, test_labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-ethnic",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "every-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punctuation = string.punctuation.replace(\"!\", \"\")\n",
    "punctuation = string.punctuation.replace(\"?\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "small-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")        \n",
    "    doc = nlp(sentence)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        elif token.pos_ == \"NUM\":\n",
    "            lemmas.append('#')\n",
    "        elif token.pos_ == \"SYM\":\n",
    "            continue\n",
    "        elif token.text in punctuation:\n",
    "            continue\n",
    "        elif re.search(r\"[http.*]\", token.text):\n",
    "            continue\n",
    "        else:\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "agricultural-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(topic_map):\n",
    "    rows = []\n",
    "    for topic, source_tweets in topic_map.items():\n",
    "        for source_tweet in source_tweets:\n",
    "            tokenized_source_tweet = preprocessing(source_tweet.tweet.post_content)\n",
    "            for reply in source_tweet.replies:\n",
    "                tokenized_reply = preprocessing(reply.post_content)\n",
    "                rows.append((topic, source_tweet.tweet.post_content, reply.post_content, tokenized_source_tweet, tokenized_reply, reply.external_urls, reply.category))\n",
    "    return pd.DataFrame(rows, columns=['topic', 'original_source_tweet', 'original_reply', 'source_tweet', 'reply', 'external_urls', 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-brown",
   "metadata": {},
   "source": [
    "## CountVectorizer and TfidfVectorizer feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "chemical-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_df = create_df(training_topic_to_tweets_map)\n",
    "test_tweets_df = create_df(test_topic_to_tweets_map)\n",
    "# training_reddit_df = create_df(training_topic_to_reddit_map)\n",
    "# test_reddit_df = create_df(test_topic_to_reddit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "upset-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = training_tweets_df[['reply', 'category']].values\n",
    "test_data = test_tweets_df[['reply', 'category']].values\n",
    "# training_tweets_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "studied-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[list(['france', '#', 'dead', 'hq', 'weekly', 'accord']) 'support']\n",
      " [list(['mt', '@euronews', 'france', '#', 'dead', 'hq', 'weekly', 'jews', 'nuke', 'israel'])\n",
      "  'comment']\n",
      " [list(['?', 'like', 'add', 'noise']) 'deny']\n",
      " [list(['usual']) 'comment']\n",
      " [list(['@euronews', 'crime', 'moslem', '?']) 'query']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "label_map = {'comment':0, 'support':1, 'deny':2, 'query':3}\n",
    "\n",
    "def count_vectorize(data, ngram, count_vect=None):\n",
    "    \n",
    "\n",
    "    text_data = []\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        text_data.append(' '.join(i[0]))\n",
    "        labels.append(label_map[i[1]])\n",
    "    \n",
    "    if count_vect is None:\n",
    "        count_vect = CountVectorizer(ngram_range=(ngram, ngram), token_pattern = '[a-zA-Z0-9#?!]+')\n",
    "        count_vect.fit(text_data)\n",
    "    \n",
    "    vectorized_data = count_vect.transform(text_data)\n",
    "    \n",
    "    return vectorized_data.toarray(), np.array(labels), count_vect\n",
    "    \n",
    "    # for i in text_data:\n",
    "    #     count_vect.transform()\n",
    "\n",
    "def tfidf_vectorize(data, ngram, count_vect=None):\n",
    "    \n",
    "\n",
    "    text_data = []\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        text_data.append(' '.join(i[0]))\n",
    "        labels.append(label_map[i[1]])\n",
    "    \n",
    "    if count_vect is None:\n",
    "        count_vect = TfidfVectorizer(ngram_range=(ngram, ngram), token_pattern = '[a-zA-Z0-9#?!]+')\n",
    "        \n",
    "        count_vect.fit(text_data)\n",
    "        # print(count_vect.get_feature_names_out())\n",
    "    \n",
    "    vectorized_data = count_vect.transform(text_data)\n",
    "    \n",
    "    return vectorized_data.toarray(), np.array(labels), count_vect\n",
    "\n",
    "# print(training_data[:5])\n",
    "# X_train, y_train, count_vect = tfidf_vectorize(training_data[:5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "54c76ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(y_pred, y_test, name, ngram_size = None):\n",
    "    if ngram_size is not None:\n",
    "        print(f'{name}, {ngram_size}-grams:')\n",
    "    else:\n",
    "        print(f'{name}:')\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    # print(y_pred.count(0), y_test.count(0))\n",
    "\n",
    "    \n",
    "    print(\" Classification accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\" Confusion matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "    target_names = ['comment', 'support', 'deny', 'query']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "629560ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5568, 3874), (5568,), (1066, 3874), (1066,))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "ngram_size = 1\n",
    "\n",
    "\n",
    "X_train, y_train, count_vect = tfidf_vectorize(training_data, ngram_size)\n",
    "X_test, y_test, _ = tfidf_vectorize(test_data, ngram_size, count_vect)\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0f139071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, 1-grams:\n",
      "{0: 1065, 1: 1}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.7223264540337712\n",
      " Confusion matrix: \n",
      " [[770   1   0   0]\n",
      " [147   0   0   0]\n",
      " [ 92   0   0   0]\n",
      " [ 56   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.72      1.00      0.84       771\n",
      "     support       0.00      0.00      0.00       147\n",
      "        deny       0.00      0.00      0.00        92\n",
      "       query       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.18      0.25      0.21      1066\n",
      "weighted avg       0.52      0.72      0.61      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "y_pred = MNB.predict(X_test)\n",
    "\n",
    "results(y_pred, y_test, 'Naive Bayes', ngram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "371b05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, 1-grams:\n",
      "{0: 1013, 1: 10, 2: 4, 3: 39}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.7223264540337712\n",
      " Confusion matrix: \n",
      " [[749   5   0  17]\n",
      " [143   1   0   3]\n",
      " [ 83   2   4   3]\n",
      " [ 38   2   0  16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.74      0.97      0.84       771\n",
      "     support       0.10      0.01      0.01       147\n",
      "        deny       1.00      0.04      0.08        92\n",
      "       query       0.41      0.29      0.34        56\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.56      0.33      0.32      1066\n",
      "weighted avg       0.66      0.72      0.63      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(max_iter = 1000)\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "results(y_pred, y_test, 'Logistic Regression', ngram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3c8225fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD, 1-grams:\n",
      "{0: 972, 1: 35, 2: 11, 3: 48}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.7129455909943715\n",
      " Confusion matrix: \n",
      " [[731  13   4  23]\n",
      " [136   5   2   4]\n",
      " [ 70  15   5   2]\n",
      " [ 35   2   0  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.75      0.95      0.84       771\n",
      "     support       0.14      0.03      0.05       147\n",
      "        deny       0.45      0.05      0.10        92\n",
      "       query       0.40      0.34      0.37        56\n",
      "\n",
      "    accuracy                           0.71      1066\n",
      "   macro avg       0.44      0.34      0.34      1066\n",
      "weighted avg       0.62      0.71      0.64      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SGD = SGDClassifier(max_iter=1000)\n",
    "SGD.fit(X_train, y_train)\n",
    "y_pred = SGD.predict(X_test)\n",
    "\n",
    "results(y_pred, y_test, 'SGD', ngram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6f19a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_SVC:\n",
      "{0: 928, 1: 75, 2: 21, 3: 42}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.699812382739212\n",
      " Confusion matrix: \n",
      " [[708  37   8  18]\n",
      " [129  10   3   5]\n",
      " [ 56  25  10   1]\n",
      " [ 35   3   0  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.76      0.92      0.83       771\n",
      "     support       0.13      0.07      0.09       147\n",
      "        deny       0.48      0.11      0.18        92\n",
      "       query       0.43      0.32      0.37        56\n",
      "\n",
      "    accuracy                           0.70      1066\n",
      "   macro avg       0.45      0.35      0.37      1066\n",
      "weighted avg       0.63      0.70      0.65      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_SVC = lin_clf = svm.LinearSVC()\n",
    "linear_SVC.fit(X_train, y_train)\n",
    "y_pred = linear_SVC.predict(X_test)\n",
    "\n",
    "results(y_pred, y_test, 'Linear_SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d65a7",
   "metadata": {},
   "source": [
    "## Hand crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "71442bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f98dfc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('negative-words.txt', 'r')\n",
    "negative_words = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('positive-words.txt', 'r')\n",
    "positive_words = f.read().split('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c989cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = model.wv\n",
    "# print(len(model['it']))\n",
    "\n",
    "def feature_extraction(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    features = []\n",
    "    word_embeddings = []\n",
    "    \n",
    "    neg_count = 0\n",
    "    pos_count = 0\n",
    "    number = 0\n",
    "    for token in doc:\n",
    "        if token.text in negative_words:\n",
    "            neg_count += 1\n",
    "        elif token.text in positive_words:\n",
    "            pos_count += 1\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        if token.pos_ == \"NUM\":\n",
    "            number = 1\n",
    "        # 25 features\n",
    "        elif token.text in model:\n",
    "            word_embeddings.extend(model[token.text])\n",
    "\n",
    "    # contains number\n",
    "    features.append(number)\n",
    "    \n",
    "    # contains ?\n",
    "    if token.text.find('?'):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    # contains !\n",
    "    if token.text.find('!'):\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    # negative and positive word count\n",
    "    features.append(neg_count)\n",
    "    features.append(pos_count)\n",
    "    \n",
    "    # capital ratio\n",
    "    uppers = [i for i in sentence if i.isupper()]\n",
    "    capitalratio = len(uppers)/len(sentence)\n",
    "    features.append(capitalratio)\n",
    "    \n",
    "    # length of sentence\n",
    "    features.append(len(sentence))\n",
    "    \n",
    "    features.extend(word_embeddings)\n",
    "    \n",
    "    \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7618ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_features(topic_map):\n",
    "    rows = []\n",
    "    for topic, source_tweets in topic_map.items():\n",
    "        for source_tweet in source_tweets:\n",
    "            tokenized_source_tweet = feature_extraction(source_tweet.tweet.post_content)\n",
    "            for reply in source_tweet.replies:\n",
    "                tokenized_reply = feature_extraction(reply.post_content)\n",
    "                rows.append((topic, source_tweet.tweet.post_content, reply.post_content, tokenized_source_tweet, tokenized_reply, reply.external_urls, reply.category))\n",
    "    return pd.DataFrame(rows, columns=['topic', 'original_source_tweet', 'original_reply', 'source_tweet', 'reply', 'external_urls', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "41bb7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_df_features = create_df_features(training_topic_to_tweets_map)\n",
    "test_tweets_df_features = create_df_features(test_topic_to_tweets_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bf4347bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1066, 607), (5568, 607))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    return max(list_len)\n",
    "\n",
    "\n",
    "training_data_features = training_tweets_df_features[['reply', 'category']].values\n",
    "test_data_features = test_tweets_df_features[['reply', 'category']].values\n",
    "\n",
    "X_train_features = training_data_features[:, 0]\n",
    "y_train_features = np.array([label_map[i] for i in training_data_features[:, 1]])\n",
    "\n",
    "X_test_features = test_data_features[:, 0]\n",
    "y_test_features = np.array([label_map[i] for i in test_data_features[:, 1]])\n",
    "\n",
    "max_len_train = find_max_list(X_train_features)\n",
    "max_len_test = find_max_list(X_test_features)\n",
    "\n",
    "max_len = max(max_len_train, max_len_test)\n",
    "\n",
    "X_train_pad = []\n",
    "for i in X_train_features:\n",
    "    # print(i)\n",
    "    if len(i) < max_len:\n",
    "        X_train_pad.append(i + [0]*(max_len-len(i)))\n",
    "    else:\n",
    "        X_train_pad.append(i)\n",
    "\n",
    "X_train_pad = np.array(X_train_pad)\n",
    "\n",
    "\n",
    "\n",
    "X_test_pad = []\n",
    "for i in X_test_features:\n",
    "    # print(i)\n",
    "    if len(i) < max_len:\n",
    "        X_test_pad.append(i + [0]*(max_len-len(i)))\n",
    "    else:\n",
    "        X_test_pad.append(i)\n",
    "\n",
    "X_test_pad = np.array(X_test_pad)\n",
    "\n",
    "X_test_pad.shape, X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fc31dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "{0: 1066}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.723264540337711\n",
      " Confusion matrix: \n",
      " [[771   0   0   0]\n",
      " [147   0   0   0]\n",
      " [ 92   0   0   0]\n",
      " [ 56   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.72      1.00      0.84       771\n",
      "     support       0.00      0.00      0.00       147\n",
      "        deny       0.00      0.00      0.00        92\n",
      "       query       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.18      0.25      0.21      1066\n",
      "weighted avg       0.52      0.72      0.61      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "p.fit(X_train_pad,y_train) \n",
    "\n",
    "y_pred = p.predict(X_test_pad)\n",
    "\n",
    "results(y_pred, y_test_features, 'Naive Bayes')\n",
    "# X_train_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4f5d618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "{0: 968, 1: 23, 2: 13, 3: 62}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.7166979362101313\n",
      " Confusion matrix: \n",
      " [[724  12   9  26]\n",
      " [132   9   0   6]\n",
      " [ 85   2   3   2]\n",
      " [ 27   0   1  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.75      0.94      0.83       771\n",
      "     support       0.39      0.06      0.11       147\n",
      "        deny       0.23      0.03      0.06        92\n",
      "       query       0.45      0.50      0.47        56\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.46      0.38      0.37      1066\n",
      "weighted avg       0.64      0.72      0.65      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_features = LogisticRegression(max_iter = 5000)\n",
    "LR_features.fit(X_train_pad, y_train)\n",
    "y_pred = LR_features.predict(X_test_pad)\n",
    "\n",
    "results(y_pred, y_test_features, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "17f2c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD:\n",
      "{0: 1021, 2: 1, 3: 44}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.7345215759849906\n",
      " Confusion matrix: \n",
      " [[756   0   0  15]\n",
      " [144   0   0   3]\n",
      " [ 91   0   1   0]\n",
      " [ 30   0   0  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.74      0.98      0.84       771\n",
      "     support       0.00      0.00      0.00       147\n",
      "        deny       1.00      0.01      0.02        92\n",
      "       query       0.59      0.46      0.52        56\n",
      "\n",
      "    accuracy                           0.73      1066\n",
      "   macro avg       0.58      0.36      0.35      1066\n",
      "weighted avg       0.65      0.73      0.64      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SGD_features = SGDClassifier(max_iter=1000)\n",
    "SGD_features.fit(X_train_pad, y_train)\n",
    "y_pred = SGD_features.predict(X_test_pad)\n",
    "\n",
    "results(y_pred, y_test_features, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "52c86272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_SVC:\n",
      "{0: 989, 1: 18, 2: 4, 3: 55}\n",
      "{0: 771, 1: 147, 2: 92, 3: 56}\n",
      " Classification accuracy:  0.725140712945591\n",
      " Confusion matrix: \n",
      " [[737  11   1  22]\n",
      " [137   7   1   2]\n",
      " [ 88   0   1   3]\n",
      " [ 27   0   1  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment       0.75      0.96      0.84       771\n",
      "     support       0.39      0.05      0.08       147\n",
      "        deny       0.25      0.01      0.02        92\n",
      "       query       0.51      0.50      0.50        56\n",
      "\n",
      "    accuracy                           0.73      1066\n",
      "   macro avg       0.47      0.38      0.36      1066\n",
      "weighted avg       0.64      0.73      0.65      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_SVC = svm.LinearSVC(dual=False)\n",
    "linear_SVC.fit(X_train_pad, y_train)\n",
    "y_pred = linear_SVC.predict(X_test_pad)\n",
    "\n",
    "results(y_pred, y_test_features, 'Linear_SVC')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
