{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "limiting-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 10:16:18.336560: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mihaela/opt/openmpi/lib\n",
      "2022-05-23 10:16:18.336597: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-conference",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deluxe-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(path):\n",
    "    f = open(path)\n",
    "    json_content = json.load(f)\n",
    "    f.close()\n",
    "    return json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boring-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_directory = 'datasets/rumoureval-2019-training-data/twitter-english'\n",
    "test_dataset_directory = 'datasets/rumoureval-2019-test-data/twitter-en-test-data'\n",
    "\n",
    "training_dataset_reddit_directory = 'datasets/rumoureval-2019-training-data/reddit-training-data'\n",
    "test_dataset_reddit_directory = 'datasets/rumoureval-2019-test-data/reddit-test-data'\n",
    "\n",
    "training_labels_json = 'datasets/rumoureval-2019-training-data/train-key.json'\n",
    "training_labels_json_2 = 'datasets/rumoureval-2019-training-data/dev-key.json'\n",
    "test_labels_json = 'datasets/final-eval-key.json'\n",
    "\n",
    "training_labels_dict = read_json_file(training_labels_json)['subtaskaenglish']\n",
    "training_labels_dict.update(read_json_file(training_labels_json_2)['subtaskaenglish'])\n",
    "test_labels_dict = read_json_file(test_labels_json)['subtaskaenglish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, post_content, post_id, parent_post_id=None, external_urls_count=0):\n",
    "        self.post_content = post_content\n",
    "        self.post_id = post_id\n",
    "        self.category = None\n",
    "        self.parent_post_id = parent_post_id\n",
    "        self.external_urls = external_urls_count > 0\n",
    "        self.user_metadata = None\n",
    "        \n",
    "    def add_category(self, category):\n",
    "        self.category = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changed-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceTweet:\n",
    "    def __init__(self, tweet: Tweet):\n",
    "        self.tweet = tweet\n",
    "        self.replies = []\n",
    "        \n",
    "    def add_reply(self, reply: Tweet):\n",
    "        self.replies.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "covered-aggregate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_tweets_dataset(dataset_dir_path, labels_dict):\n",
    "    topic_directories = [f.path for f in os.scandir(dataset_dir_path) if f.is_dir()]\n",
    "    topic_to_tweets_map = {}  # {topic_name: [SourceTweet, ...]}\n",
    "\n",
    "    for topic_dir in topic_directories:\n",
    "        topic_name = topic_dir.split('/')[1]\n",
    "        source_tweets = []\n",
    "        \n",
    "        tweets_paths = [f.path for f in os.scandir(topic_dir) if f.is_dir()]\n",
    "        for tweet_dir in tweets_paths:\n",
    "            source_tweet_path = [f.path for f in os.scandir(tweet_dir + '/source-tweet')][0]\n",
    "            source_tweet_json = read_json_file(source_tweet_path)\n",
    "            \n",
    "            tweet = Tweet(source_tweet_json['text'], source_tweet_json['id'],\n",
    "                              source_tweet_json['in_reply_to_status_id'],\n",
    "                              len(source_tweet_json['entities']['urls']))\n",
    "            \n",
    "            source_tweet = SourceTweet(tweet)\n",
    "            source_tweets.append(source_tweet)\n",
    "            tweet.add_category(\"support\")\n",
    "            source_tweet.add_reply(tweet)\n",
    "\n",
    "            #if not os.path.exists(topic_dir+'replies'):\n",
    "            #    continue\n",
    "            try:\n",
    "                reply_tweets_paths = [f.path for f in os.scandir(tweet_dir + '/replies')]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            for reply_tweet_path in reply_tweets_paths:\n",
    "                reply_tweet_json = read_json_file(reply_tweet_path)\n",
    "                \n",
    "                reply_tweet = Tweet(reply_tweet_json['text'], reply_tweet_json['id'],\n",
    "                                        source_tweet.tweet.post_id, len(reply_tweet_json['entities']['urls']))\n",
    "                reply_tweet.add_category(labels_dict[str(reply_tweet_json['id'])])\n",
    "                source_tweet.add_reply(reply_tweet)\n",
    "        \n",
    "        topic_to_tweets_map[topic_name] = source_tweets\n",
    "        \n",
    "    return topic_to_tweets_map\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "individual-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reddit_dataset(dataset_dir_path, labels_dict):\n",
    "    topic_directories = [f.path for f in os.scandir(dataset_dir_path) if f.is_dir()]\n",
    "    topic_to_tweets_map = {}  # {topic_name: [SourceTweet, ...]}\n",
    "\n",
    "    for topic_dir in topic_directories:\n",
    "        topic_name = topic_dir.split('/')[1]\n",
    "        source_tweets = []\n",
    "        \n",
    "        source_tweet_path = [f.path for f in os.scandir(topic_dir + '/source-tweet')][0]\n",
    "        source_tweet_json = read_json_file(source_tweet_path)\n",
    "\n",
    "        content = source_tweet_json['data']['children'][0]['data']['title'] + ' ' + source_tweet_json['data']['children'][0]['data']['selftext']\n",
    "        tweet = Tweet(content, source_tweet_json['data']['children'][0]['data']['id'], None, content.count(\"http\"))\n",
    "\n",
    "        source_tweet = SourceTweet(tweet)\n",
    "        source_tweets.append(source_tweet)\n",
    "        tweet.add_category(\"support\")\n",
    "        source_tweet.add_reply(tweet)\n",
    "\n",
    "        #if not os.path.exists(topic_dir+'replies'):\n",
    "        #    continue\n",
    "\n",
    "        try:\n",
    "            reply_tweets_paths = [f.path for f in os.scandir(topic_dir + '/replies')]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        for reply_tweet_path in reply_tweets_paths:\n",
    "            reply_tweet_json = read_json_file(reply_tweet_path)\n",
    "            \n",
    "            if 'body' in reply_tweet_json['data']:\n",
    "                reply_tweet = Tweet(reply_tweet_json['data']['body'], reply_tweet_json['data']['id'],\n",
    "                                        source_tweet.tweet.post_id, reply_tweet_json['data']['body'].count('http'))\n",
    "                reply_tweet.add_category(labels_dict[str(reply_tweet.post_id)])\n",
    "                source_tweet.add_reply(reply_tweet)\n",
    "                \n",
    "        topic_to_tweets_map[topic_name] = source_tweets\n",
    "        \n",
    "    return topic_to_tweets_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "driven-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "training_topic_to_tweets_map = read_tweets_dataset(training_dataset_directory, training_labels_dict)\n",
    "test_topic_to_tweets_map = read_tweets_dataset(test_dataset_directory, test_labels_dict)\n",
    "\n",
    "# Reddit\n",
    "training_topic_to_reddit_map = read_reddit_dataset(training_dataset_reddit_directory, training_labels_dict)\n",
    "test_topic_to_reddit_map = read_reddit_dataset(test_dataset_reddit_directory, test_labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-ethnic",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "every-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "small-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")        \n",
    "    doc = nlp(sentence)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        elif token.pos_ == \"NUM\":\n",
    "            lemmas.append('#')\n",
    "        elif token.pos_ == \"SYM\":\n",
    "            continue\n",
    "        else:\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "agricultural-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(topic_map):\n",
    "    rows = []\n",
    "    for topic, source_tweets in topic_map.items():\n",
    "        for source_tweet in source_tweets:\n",
    "            tokenized_source_tweet = preprocessing(source_tweet.tweet.post_content)\n",
    "            for reply in source_tweet.replies:\n",
    "                tokenized_reply = preprocessing(reply.post_content)\n",
    "                rows.append((topic, source_tweet.tweet.post_content, reply.post_content, tokenized_source_tweet, tokenized_reply, reply.external_urls, reply.category))\n",
    "    return pd.DataFrame(rows, columns=['topic', 'original_source_tweet', 'original_reply', 'source_tweet', 'reply', 'external_urls', 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-brown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chemical-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_df = create_df(training_topic_to_tweets_map)\n",
    "test_tweets_df = create_df(test_topic_to_tweets_map)\n",
    "training_reddit_df = create_df(training_topic_to_reddit_map)\n",
    "test_reddit_df = create_df(test_topic_to_reddit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upset-coordinate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>original_source_tweet</th>\n",
       "      <th>original_reply</th>\n",
       "      <th>source_tweet</th>\n",
       "      <th>reply</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>@brofromanother @LiveNationON @masseyhall WHAT...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>[@brofromanother, @livenationon, @masseyhall, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>@LiveNationON Soo... Its tomorrow night!? #Pri...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>[@livenationon, soo, ..., tomorrow, night, !, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>There ya go. @KiSS925 RT @LiveNationON: In res...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>[ya, ., @kiss925, rt, @livenationon, :, respon...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>In response to inquiries, we can confirm that ...</td>\n",
       "      <td>@LiveNationON @masseyhall it's probably @3RDEY...</td>\n",
       "      <td>[response, inquiry, ,, confirm, prince, perfor...</td>\n",
       "      <td>[@livenationon, @masseyhall, probably, @3rdeye...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>@NobleRobel Girl, don't be mean.   Age ain't n...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>[@noblerobel, girl, ,, mean, .,   , age, be, n...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>@mikeyerxa you should send Prince a set of ten...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>[@mikeyerxa, send, prince, set, tennis, ball, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>@mikeyerxa EXCUSE ME, WHERE'S MY INVITE (to th...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>[@mikeyerxa, excuse, ,, invite, (, ,, duh, )]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Prince is playing a secret show in Toronto ton...</td>\n",
       "      <td>@halihamilton @mikeyerxa gonna try to make it ...</td>\n",
       "      <td>[prince, play, secret, toronto, tonight, .,  ,...</td>\n",
       "      <td>[@halihamilton, @mikeyerxa, go, to, try, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             topic  \\\n",
       "0    rumoureval-2019-training-data   \n",
       "1    rumoureval-2019-training-data   \n",
       "2    rumoureval-2019-training-data   \n",
       "3    rumoureval-2019-training-data   \n",
       "4    rumoureval-2019-training-data   \n",
       "..                             ...   \n",
       "98   rumoureval-2019-training-data   \n",
       "99   rumoureval-2019-training-data   \n",
       "100  rumoureval-2019-training-data   \n",
       "101  rumoureval-2019-training-data   \n",
       "102  rumoureval-2019-training-data   \n",
       "\n",
       "                                 original_source_tweet  \\\n",
       "0    In response to inquiries, we can confirm that ...   \n",
       "1    In response to inquiries, we can confirm that ...   \n",
       "2    In response to inquiries, we can confirm that ...   \n",
       "3    In response to inquiries, we can confirm that ...   \n",
       "4    In response to inquiries, we can confirm that ...   \n",
       "..                                                 ...   \n",
       "98   Prince is playing a secret show in Toronto ton...   \n",
       "99   Prince is playing a secret show in Toronto ton...   \n",
       "100  Prince is playing a secret show in Toronto ton...   \n",
       "101  Prince is playing a secret show in Toronto ton...   \n",
       "102  Prince is playing a secret show in Toronto ton...   \n",
       "\n",
       "                                        original_reply  \\\n",
       "0    In response to inquiries, we can confirm that ...   \n",
       "1    @brofromanother @LiveNationON @masseyhall WHAT...   \n",
       "2    @LiveNationON Soo... Its tomorrow night!? #Pri...   \n",
       "3    There ya go. @KiSS925 RT @LiveNationON: In res...   \n",
       "4    @LiveNationON @masseyhall it's probably @3RDEY...   \n",
       "..                                                 ...   \n",
       "98   Prince is playing a secret show in Toronto ton...   \n",
       "99   @NobleRobel Girl, don't be mean.   Age ain't n...   \n",
       "100  @mikeyerxa you should send Prince a set of ten...   \n",
       "101  @mikeyerxa EXCUSE ME, WHERE'S MY INVITE (to th...   \n",
       "102  @halihamilton @mikeyerxa gonna try to make it ...   \n",
       "\n",
       "                                          source_tweet  \\\n",
       "0    [response, inquiry, ,, confirm, prince, perfor...   \n",
       "1    [response, inquiry, ,, confirm, prince, perfor...   \n",
       "2    [response, inquiry, ,, confirm, prince, perfor...   \n",
       "3    [response, inquiry, ,, confirm, prince, perfor...   \n",
       "4    [response, inquiry, ,, confirm, prince, perfor...   \n",
       "..                                                 ...   \n",
       "98   [prince, play, secret, toronto, tonight, .,  ,...   \n",
       "99   [prince, play, secret, toronto, tonight, .,  ,...   \n",
       "100  [prince, play, secret, toronto, tonight, .,  ,...   \n",
       "101  [prince, play, secret, toronto, tonight, .,  ,...   \n",
       "102  [prince, play, secret, toronto, tonight, .,  ,...   \n",
       "\n",
       "                                                 reply  external_urls category  \n",
       "0    [response, inquiry, ,, confirm, prince, perfor...          False  support  \n",
       "1    [@brofromanother, @livenationon, @masseyhall, ...          False  comment  \n",
       "2    [@livenationon, soo, ..., tomorrow, night, !, ...          False    query  \n",
       "3    [ya, ., @kiss925, rt, @livenationon, :, respon...          False  comment  \n",
       "4    [@livenationon, @masseyhall, probably, @3rdeye...          False  comment  \n",
       "..                                                 ...            ...      ...  \n",
       "98   [prince, play, secret, toronto, tonight, .,  ,...          False  support  \n",
       "99   [@noblerobel, girl, ,, mean, .,   , age, be, n...          False  comment  \n",
       "100  [@mikeyerxa, send, prince, set, tennis, ball, ...          False  comment  \n",
       "101      [@mikeyerxa, excuse, ,, invite, (, ,, duh, )]          False  comment  \n",
       "102        [@halihamilton, @mikeyerxa, go, to, try, .]          False  comment  \n",
       "\n",
       "[103 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "studied-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>original_source_tweet</th>\n",
       "      <th>original_reply</th>\n",
       "      <th>source_tweet</th>\n",
       "      <th>reply</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>True</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor Article is dated in October?  It's a...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, article, date, october, ?,  , para...</td>\n",
       "      <td>True</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor @GrizzleMeister Great Example, CaGov...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, @grizzlemeister, great, example, ,...</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor I’m sure the sanctuary state will fo...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, sure, sanctuary, state, forgive, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor FAKE...you gotta do better Janie.</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, fake, ..., get, to, well, janie, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor @GrizzleMeister Why am I not surpris...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, @grizzlemeister, surprised, ,, cou...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor @GrizzleMeister Couldn't get past th...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, @grizzlemeister, past, ad, read]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@jjauthor This makes one wonder about the orig...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@jjauthor, make, wonder, origin, fire, past, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>True</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe shut down #MSA &amp;amp; visa's</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, shut, msa, &amp;, amp, ;, visa]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe No source cited in this article, no ...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, source, cite, article, ,, date, .....</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Hey Doc, does this story have altern...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, hey, doc, ,, story, alternative, c...</td>\n",
       "      <td>False</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Everything about this is fake. Could...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, fake, ., come, w, well, ?, \", ghos...</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe @manm8ing Well as The Lord says . \" ...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, @manm8e, lord, say, ., \", reap, so...</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Roseanne, records seized in Bin Lade...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, roseanne, ,, record, seize, bin, l...</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Fake news.</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, fake, news, .]</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe @Cindylevy444 And don't forget the f...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, @cindylevy444, forget, fire, napa,...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe This is a bot it does not have the T...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, bot, twitter, bird]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Well at least he is safe in sanctuar...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, safe, sanctuary]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe @deenie7940 What happen the the ille...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, @deenie7940, happen, illegal, star...</td>\n",
       "      <td>False</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Hmm illegals doing horrible things i...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, hmm, illegal, horrible, thing, san...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe No way. Say it isnt so. Fires start ...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, way, ., not, ., fire, start, time,...</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe How ironic</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, ironic]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe But...but...but Governor Brown said ...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, ..., ..., governor, brown, say, gl...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe What is this garbage? #fakenews</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, garbage, ?, #, fakenew]</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe @DanicaPfan7 If this is true? They r...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, @danicapfan7, true, ?, r, stupid, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe @barkingstarss Fake. Do research bef...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, @barkingstarss, fake, ., research,...</td>\n",
       "      <td>True</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe https://t.co/GglF6jpEuZ</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, https://t.co/gglf6jpeuz]</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>BREAKING: Illegal Muslim From Iran Arrested Fo...</td>\n",
       "      <td>@WhoWolfe Will his defense be Jerry Brown said...</td>\n",
       "      <td>[breaking, :, illegal, muslim, iran, arrest, s...</td>\n",
       "      <td>[@whowolfe, defense, jerry, brown, say, fire, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic  \\\n",
       "0   rumoureval-2019-test-data   \n",
       "1   rumoureval-2019-test-data   \n",
       "2   rumoureval-2019-test-data   \n",
       "3   rumoureval-2019-test-data   \n",
       "4   rumoureval-2019-test-data   \n",
       "5   rumoureval-2019-test-data   \n",
       "6   rumoureval-2019-test-data   \n",
       "7   rumoureval-2019-test-data   \n",
       "8   rumoureval-2019-test-data   \n",
       "9   rumoureval-2019-test-data   \n",
       "10  rumoureval-2019-test-data   \n",
       "11  rumoureval-2019-test-data   \n",
       "12  rumoureval-2019-test-data   \n",
       "13  rumoureval-2019-test-data   \n",
       "14  rumoureval-2019-test-data   \n",
       "15  rumoureval-2019-test-data   \n",
       "16  rumoureval-2019-test-data   \n",
       "17  rumoureval-2019-test-data   \n",
       "18  rumoureval-2019-test-data   \n",
       "19  rumoureval-2019-test-data   \n",
       "20  rumoureval-2019-test-data   \n",
       "21  rumoureval-2019-test-data   \n",
       "22  rumoureval-2019-test-data   \n",
       "23  rumoureval-2019-test-data   \n",
       "24  rumoureval-2019-test-data   \n",
       "25  rumoureval-2019-test-data   \n",
       "26  rumoureval-2019-test-data   \n",
       "27  rumoureval-2019-test-data   \n",
       "28  rumoureval-2019-test-data   \n",
       "\n",
       "                                original_source_tweet  \\\n",
       "0   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "1   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "2   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "3   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "4   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "5   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "6   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "7   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "8   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "9   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "10  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "11  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "12  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "13  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "14  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "15  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "16  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "17  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "18  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "19  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "20  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "21  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "22  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "23  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "24  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "25  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "26  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "27  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "28  BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "\n",
       "                                       original_reply  \\\n",
       "0   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "1   @jjauthor Article is dated in October?  It's a...   \n",
       "2   @jjauthor @GrizzleMeister Great Example, CaGov...   \n",
       "3   @jjauthor I’m sure the sanctuary state will fo...   \n",
       "4         @jjauthor FAKE...you gotta do better Janie.   \n",
       "5   @jjauthor @GrizzleMeister Why am I not surpris...   \n",
       "6   @jjauthor @GrizzleMeister Couldn't get past th...   \n",
       "7   @jjauthor This makes one wonder about the orig...   \n",
       "8   BREAKING: Illegal Muslim From Iran Arrested Fo...   \n",
       "9               @WhoWolfe shut down #MSA &amp; visa's   \n",
       "10  @WhoWolfe No source cited in this article, no ...   \n",
       "11  @WhoWolfe Hey Doc, does this story have altern...   \n",
       "12  @WhoWolfe Everything about this is fake. Could...   \n",
       "13  @WhoWolfe @manm8ing Well as The Lord says . \" ...   \n",
       "14  @WhoWolfe Roseanne, records seized in Bin Lade...   \n",
       "15                               @WhoWolfe Fake news.   \n",
       "16  @WhoWolfe @Cindylevy444 And don't forget the f...   \n",
       "17  @WhoWolfe This is a bot it does not have the T...   \n",
       "18  @WhoWolfe Well at least he is safe in sanctuar...   \n",
       "19  @WhoWolfe @deenie7940 What happen the the ille...   \n",
       "20  @WhoWolfe Hmm illegals doing horrible things i...   \n",
       "21  @WhoWolfe No way. Say it isnt so. Fires start ...   \n",
       "22                               @WhoWolfe How ironic   \n",
       "23  @WhoWolfe But...but...but Governor Brown said ...   \n",
       "24          @WhoWolfe What is this garbage? #fakenews   \n",
       "25  @WhoWolfe @DanicaPfan7 If this is true? They r...   \n",
       "26  @WhoWolfe @barkingstarss Fake. Do research bef...   \n",
       "27                  @WhoWolfe https://t.co/GglF6jpEuZ   \n",
       "28  @WhoWolfe Will his defense be Jerry Brown said...   \n",
       "\n",
       "                                         source_tweet  \\\n",
       "0   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "1   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "2   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "3   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "4   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "5   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "6   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "7   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "8   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "9   [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "10  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "11  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "12  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "13  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "14  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "15  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "16  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "17  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "18  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "19  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "20  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "21  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "22  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "23  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "24  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "25  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "26  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "27  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "28  [breaking, :, illegal, muslim, iran, arrest, s...   \n",
       "\n",
       "                                                reply  external_urls category  \n",
       "0   [breaking, :, illegal, muslim, iran, arrest, s...           True  support  \n",
       "1   [@jjauthor, article, date, october, ?,  , para...           True     deny  \n",
       "2   [@jjauthor, @grizzlemeister, great, example, ,...           True  comment  \n",
       "3     [@jjauthor, sure, sanctuary, state, forgive, .]          False  comment  \n",
       "4     [@jjauthor, fake, ..., get, to, well, janie, .]          False  comment  \n",
       "5   [@jjauthor, @grizzlemeister, surprised, ,, cou...          False  support  \n",
       "6        [@jjauthor, @grizzlemeister, past, ad, read]          False  comment  \n",
       "7   [@jjauthor, make, wonder, origin, fire, past, ...          False  comment  \n",
       "8   [breaking, :, illegal, muslim, iran, arrest, s...           True  support  \n",
       "9             [@whowolfe, shut, msa, &, amp, ;, visa]          False  comment  \n",
       "10  [@whowolfe, source, cite, article, ,, date, .....          False     deny  \n",
       "11  [@whowolfe, hey, doc, ,, story, alternative, c...          False    query  \n",
       "12  [@whowolfe, fake, ., come, w, well, ?, \", ghos...          False     deny  \n",
       "13  [@whowolfe, @manm8e, lord, say, ., \", reap, so...           True  comment  \n",
       "14  [@whowolfe, roseanne, ,, record, seize, bin, l...           True  comment  \n",
       "15                         [@whowolfe, fake, news, .]          False     deny  \n",
       "16  [@whowolfe, @cindylevy444, forget, fire, napa,...          False  comment  \n",
       "17                    [@whowolfe, bot, twitter, bird]          False  comment  \n",
       "18                       [@whowolfe, safe, sanctuary]          False  comment  \n",
       "19  [@whowolfe, @deenie7940, happen, illegal, star...          False    query  \n",
       "20  [@whowolfe, hmm, illegal, horrible, thing, san...          False  comment  \n",
       "21  [@whowolfe, way, ., not, ., fire, start, time,...          False     deny  \n",
       "22                                [@whowolfe, ironic]          False  comment  \n",
       "23  [@whowolfe, ..., ..., governor, brown, say, gl...          False  comment  \n",
       "24                [@whowolfe, garbage, ?, #, fakenew]          False     deny  \n",
       "25  [@whowolfe, @danicapfan7, true, ?, r, stupid, ...           True    query  \n",
       "26  [@whowolfe, @barkingstarss, fake, ., research,...           True     deny  \n",
       "27               [@whowolfe, https://t.co/gglf6jpeuz]           True  comment  \n",
       "28  [@whowolfe, defense, jerry, brown, say, fire, ...          False    query  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "macro-affair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>original_source_tweet</th>\n",
       "      <th>original_reply</th>\n",
       "      <th>source_tweet</th>\n",
       "      <th>reply</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>&amp;gt;It has been known for some years that the ...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[&amp;, gt;it, know, year, problem, microwaved, ra...</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>Where was her control ?, why did she prune new...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[control, ?, ,, prune, newly, grow, plant, ?, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>Also different antigens in blood become more o...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[different, antigen, blood, active, different,...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>[This has been debunked by Snopes](http://www....</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[[, debunk, snopes](http://www.snopes.com, sci...</td>\n",
       "      <td>True</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>Mythbusters tackled the plant thing and found ...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[mythbuster, tackle, plant, thing, find, bust,...</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>1. Microwaved water is just normal water, but ...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[1, ., microwaved, water, normal, water, ,, ho...</td>\n",
       "      <td>False</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>&amp;gt;The body doesn't give a tinkers cuss about...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[&amp;, gt;the, body, tinker, cuss, dna, food, ing...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rumoureval-2019-training-data</td>\n",
       "      <td>Debunk This: Microwaves are bad because microw...</td>\n",
       "      <td>Apparently her control was another plant which...</td>\n",
       "      <td>[debunk, :, microwave, bad, microwaved, water,...</td>\n",
       "      <td>[apparently, control, plant, water, water, boi...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           topic  \\\n",
       "0  rumoureval-2019-training-data   \n",
       "1  rumoureval-2019-training-data   \n",
       "2  rumoureval-2019-training-data   \n",
       "3  rumoureval-2019-training-data   \n",
       "4  rumoureval-2019-training-data   \n",
       "5  rumoureval-2019-training-data   \n",
       "6  rumoureval-2019-training-data   \n",
       "7  rumoureval-2019-training-data   \n",
       "8  rumoureval-2019-training-data   \n",
       "\n",
       "                               original_source_tweet  \\\n",
       "0  Debunk This: Microwaves are bad because microw...   \n",
       "1  Debunk This: Microwaves are bad because microw...   \n",
       "2  Debunk This: Microwaves are bad because microw...   \n",
       "3  Debunk This: Microwaves are bad because microw...   \n",
       "4  Debunk This: Microwaves are bad because microw...   \n",
       "5  Debunk This: Microwaves are bad because microw...   \n",
       "6  Debunk This: Microwaves are bad because microw...   \n",
       "7  Debunk This: Microwaves are bad because microw...   \n",
       "8  Debunk This: Microwaves are bad because microw...   \n",
       "\n",
       "                                      original_reply  \\\n",
       "0  Debunk This: Microwaves are bad because microw...   \n",
       "1  &gt;It has been known for some years that the ...   \n",
       "2  Where was her control ?, why did she prune new...   \n",
       "3  Also different antigens in blood become more o...   \n",
       "4  [This has been debunked by Snopes](http://www....   \n",
       "5  Mythbusters tackled the plant thing and found ...   \n",
       "6  1. Microwaved water is just normal water, but ...   \n",
       "7  &gt;The body doesn't give a tinkers cuss about...   \n",
       "8  Apparently her control was another plant which...   \n",
       "\n",
       "                                        source_tweet  \\\n",
       "0  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "1  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "2  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "3  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "4  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "5  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "6  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "7  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "8  [debunk, :, microwave, bad, microwaved, water,...   \n",
       "\n",
       "                                               reply  external_urls category  \n",
       "0  [debunk, :, microwave, bad, microwaved, water,...          False  support  \n",
       "1  [&, gt;it, know, year, problem, microwaved, ra...          False     deny  \n",
       "2  [control, ?, ,, prune, newly, grow, plant, ?, ...          False    query  \n",
       "3  [different, antigen, blood, active, different,...          False  comment  \n",
       "4  [[, debunk, snopes](http://www.snopes.com, sci...           True     deny  \n",
       "5  [mythbuster, tackle, plant, thing, find, bust,...          False     deny  \n",
       "6  [1, ., microwaved, water, normal, water, ,, ho...          False     deny  \n",
       "7  [&, gt;the, body, tinker, cuss, dna, food, ing...          False  comment  \n",
       "8  [apparently, control, plant, water, water, boi...          False  comment  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dedicated-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>original_source_tweet</th>\n",
       "      <th>original_reply</th>\n",
       "      <th>source_tweet</th>\n",
       "      <th>reply</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>A girl I dated in college for a bit was electe...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[girl, date, college, bit, elect, su, presiden...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>I think we had ~15% for an NUS disaffiliation ...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[think, ~15, %, nus, disaffiliation, referendu...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>Well while I am sure this is a vocal minority ...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[sure, vocal, minority, need, stop, judge, peo...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>I hate Gladstone, and even I think this is non...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[hate, gladstone, ,, think, nonsense, high, or...</td>\n",
       "      <td>False</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>If you are campaigning for this then you're no...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[campaign, liberal, ., try, -, write, history,...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>You can and probably should, but student polit...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[probably, ,, student, politician, real, polit...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>Mostly on the dodgy side of the web (download ...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[dodgy, web, (, download, site, link, ), ,, we...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>So?</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[?]</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>rumoureval-2019-test-data</td>\n",
       "      <td>Liverpool students campaign to remove Gladston...</td>\n",
       "      <td>I don't know about majority, but I think they ...</td>\n",
       "      <td>[liverpool, student, campaign, remove, gladsto...</td>\n",
       "      <td>[know, majority, ,, think, afraid, vocal, look...</td>\n",
       "      <td>False</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         topic  \\\n",
       "0    rumoureval-2019-test-data   \n",
       "1    rumoureval-2019-test-data   \n",
       "2    rumoureval-2019-test-data   \n",
       "3    rumoureval-2019-test-data   \n",
       "4    rumoureval-2019-test-data   \n",
       "..                         ...   \n",
       "192  rumoureval-2019-test-data   \n",
       "193  rumoureval-2019-test-data   \n",
       "194  rumoureval-2019-test-data   \n",
       "195  rumoureval-2019-test-data   \n",
       "196  rumoureval-2019-test-data   \n",
       "\n",
       "                                 original_source_tweet  \\\n",
       "0    Liverpool students campaign to remove Gladston...   \n",
       "1    Liverpool students campaign to remove Gladston...   \n",
       "2    Liverpool students campaign to remove Gladston...   \n",
       "3    Liverpool students campaign to remove Gladston...   \n",
       "4    Liverpool students campaign to remove Gladston...   \n",
       "..                                                 ...   \n",
       "192  Liverpool students campaign to remove Gladston...   \n",
       "193  Liverpool students campaign to remove Gladston...   \n",
       "194  Liverpool students campaign to remove Gladston...   \n",
       "195  Liverpool students campaign to remove Gladston...   \n",
       "196  Liverpool students campaign to remove Gladston...   \n",
       "\n",
       "                                        original_reply  \\\n",
       "0    Liverpool students campaign to remove Gladston...   \n",
       "1    A girl I dated in college for a bit was electe...   \n",
       "2    I think we had ~15% for an NUS disaffiliation ...   \n",
       "3    Well while I am sure this is a vocal minority ...   \n",
       "4    I hate Gladstone, and even I think this is non...   \n",
       "..                                                 ...   \n",
       "192  If you are campaigning for this then you're no...   \n",
       "193  You can and probably should, but student polit...   \n",
       "194  Mostly on the dodgy side of the web (download ...   \n",
       "195                                                So?   \n",
       "196  I don't know about majority, but I think they ...   \n",
       "\n",
       "                                          source_tweet  \\\n",
       "0    [liverpool, student, campaign, remove, gladsto...   \n",
       "1    [liverpool, student, campaign, remove, gladsto...   \n",
       "2    [liverpool, student, campaign, remove, gladsto...   \n",
       "3    [liverpool, student, campaign, remove, gladsto...   \n",
       "4    [liverpool, student, campaign, remove, gladsto...   \n",
       "..                                                 ...   \n",
       "192  [liverpool, student, campaign, remove, gladsto...   \n",
       "193  [liverpool, student, campaign, remove, gladsto...   \n",
       "194  [liverpool, student, campaign, remove, gladsto...   \n",
       "195  [liverpool, student, campaign, remove, gladsto...   \n",
       "196  [liverpool, student, campaign, remove, gladsto...   \n",
       "\n",
       "                                                 reply  external_urls category  \n",
       "0    [liverpool, student, campaign, remove, gladsto...          False  support  \n",
       "1    [girl, date, college, bit, elect, su, presiden...          False  comment  \n",
       "2    [think, ~15, %, nus, disaffiliation, referendu...          False  comment  \n",
       "3    [sure, vocal, minority, need, stop, judge, peo...          False  comment  \n",
       "4    [hate, gladstone, ,, think, nonsense, high, or...          False  support  \n",
       "..                                                 ...            ...      ...  \n",
       "192  [campaign, liberal, ., try, -, write, history,...          False  comment  \n",
       "193  [probably, ,, student, politician, real, polit...          False  comment  \n",
       "194  [dodgy, web, (, download, site, link, ), ,, we...          False  comment  \n",
       "195                                                [?]          False  comment  \n",
       "196  [know, majority, ,, think, afraid, vocal, look...          False  comment  \n",
       "\n",
       "[197 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reddit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3213a",
   "metadata": {},
   "source": [
    "## LSTM network for stance classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd78790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Bidirectional\n",
    "import keras.preprocessing.sequence as seq\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f921fd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      In response to inquiries, we can confirm that ...\n",
       "1      In response to inquiries, we can confirm that ...\n",
       "2      In response to inquiries, we can confirm that ...\n",
       "3      In response to inquiries, we can confirm that ...\n",
       "4      In response to inquiries, we can confirm that ...\n",
       "                             ...                        \n",
       "98     Prince is playing a secret show in Toronto ton...\n",
       "99     Prince is playing a secret show in Toronto ton...\n",
       "100    Prince is playing a secret show in Toronto ton...\n",
       "101    Prince is playing a secret show in Toronto ton...\n",
       "102    Prince is playing a secret show in Toronto ton...\n",
       "Name: original_source_tweet, Length: 103, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets_df['original_source_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239bbfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      In response to inquiries, we can confirm that ...\n",
       "1      @brofromanother @LiveNationON @masseyhall WHAT...\n",
       "2      @LiveNationON Soo... Its tomorrow night!? #Pri...\n",
       "3      There ya go. @KiSS925 RT @LiveNationON: In res...\n",
       "4      @LiveNationON @masseyhall it's probably @3RDEY...\n",
       "                             ...                        \n",
       "98     Prince is playing a secret show in Toronto ton...\n",
       "99     @NobleRobel Girl, don't be mean.   Age ain't n...\n",
       "100    @mikeyerxa you should send Prince a set of ten...\n",
       "101    @mikeyerxa EXCUSE ME, WHERE'S MY INVITE (to th...\n",
       "102    @halihamilton @mikeyerxa gonna try to make it ...\n",
       "Name: original_reply, Length: 103, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tweets_df['original_reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f06796b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      In response to inquiries, we can confirm that ...\n",
      "1      In response to inquiries, we can confirm that ...\n",
      "2      In response to inquiries, we can confirm that ...\n",
      "3      In response to inquiries, we can confirm that ...\n",
      "4      In response to inquiries, we can confirm that ...\n",
      "                             ...                        \n",
      "98     Prince is playing a secret show in Toronto ton...\n",
      "99     Prince is playing a secret show in Toronto ton...\n",
      "100    Prince is playing a secret show in Toronto ton...\n",
      "101    Prince is playing a secret show in Toronto ton...\n",
      "102    Prince is playing a secret show in Toronto ton...\n",
      "Name: source_reply, Length: 103, dtype: object\n",
      "0     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "1     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "2     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "3     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "4     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "5     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "6     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "7     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "8     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "9     BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "10    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "11    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "12    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "13    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "14    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "15    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "16    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "17    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "18    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "19    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "20    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "21    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "22    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "23    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "24    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "25    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "26    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "27    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "28    BREAKING: Illegal Muslim From Iran Arrested Fo...\n",
      "Name: source_reply, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# concatenating tweet input and tweet replies\n",
    "training_tweets_df['source_reply'] = training_tweets_df['original_source_tweet'] + training_tweets_df['original_reply']\n",
    "test_tweets_df['source_reply'] = test_tweets_df['original_source_tweet'] + test_tweets_df['original_reply']\n",
    "\n",
    "print(training_tweets_df['source_reply'])\n",
    "print(test_tweets_df['source_reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a7ddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  14  15   8]\n",
      " [  0   0   0 ... 209 210  53]\n",
      " [  0   0   0 ... 116   1  15]\n",
      " ...\n",
      " [  0   0   0 ... 487 173 488]\n",
      " [  0   0   0 ...  38 492 493]\n",
      " [  0   0   0 ...  37   7 497]]\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = 300000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(training_tweets_df['source_reply'])\n",
    "train_sequences = tokenizer.texts_to_sequences(training_tweets_df['source_reply'])\n",
    "\n",
    "tokenizer.fit_on_texts(training_tweets_df['source_reply'])\n",
    "test_sequences = tokenizer.texts_to_sequences(training_tweets_df['source_reply'])\n",
    "\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=80, dtype='int32')\n",
    "\n",
    "encoder_train = LabelEncoder()\n",
    "encoder_train.fit(training_tweets_df['category'])\n",
    "y_train = np_utils.to_categorical(encoder_train.transform(training_tweets_df['category']))\n",
    "\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "327f87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 80, 100)           2000000   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 200)              160800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,161,604\n",
      "Trainable params: 2,161,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 6s 855ms/step - loss: 1.3841 - categorical_accuracy: 0.2951 - val_loss: 1.3366 - val_categorical_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 288ms/step - loss: 1.3371 - categorical_accuracy: 0.5246 - val_loss: 1.2690 - val_categorical_accuracy: 0.7143\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 275ms/step - loss: 1.2836 - categorical_accuracy: 0.5246 - val_loss: 1.1825 - val_categorical_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 1.2281 - categorical_accuracy: 0.5246 - val_loss: 1.0482 - val_categorical_accuracy: 0.7143\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 279ms/step - loss: 1.1540 - categorical_accuracy: 0.5246 - val_loss: 0.8882 - val_categorical_accuracy: 0.7143\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 1.1527 - categorical_accuracy: 0.5246 - val_loss: 0.9021 - val_categorical_accuracy: 0.7143\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 1s 268ms/step - loss: 1.1222 - categorical_accuracy: 0.5246 - val_loss: 0.9352 - val_categorical_accuracy: 0.7143\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 289ms/step - loss: 1.0966 - categorical_accuracy: 0.5246 - val_loss: 0.9789 - val_categorical_accuracy: 0.7143\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 279ms/step - loss: 1.0780 - categorical_accuracy: 0.5246 - val_loss: 1.0017 - val_categorical_accuracy: 0.7143\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 1.0655 - categorical_accuracy: 0.5246 - val_loss: 0.9914 - val_categorical_accuracy: 0.7143\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 1.0518 - categorical_accuracy: 0.5246 - val_loss: 0.9738 - val_categorical_accuracy: 0.7143\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 1.0144 - categorical_accuracy: 0.5246 - val_loss: 1.0088 - val_categorical_accuracy: 0.7143\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.9838 - categorical_accuracy: 0.5246 - val_loss: 1.0106 - val_categorical_accuracy: 0.7143\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.9464 - categorical_accuracy: 0.5246 - val_loss: 0.9984 - val_categorical_accuracy: 0.7143\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.9207 - categorical_accuracy: 0.5246 - val_loss: 1.0010 - val_categorical_accuracy: 0.7143\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.9019 - categorical_accuracy: 0.5574 - val_loss: 1.0122 - val_categorical_accuracy: 0.7143\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 273ms/step - loss: 0.8926 - categorical_accuracy: 0.6393 - val_loss: 1.0517 - val_categorical_accuracy: 0.7143\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.8410 - categorical_accuracy: 0.6557 - val_loss: 1.0652 - val_categorical_accuracy: 0.7381\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 278ms/step - loss: 0.8161 - categorical_accuracy: 0.7049 - val_loss: 1.0560 - val_categorical_accuracy: 0.7381\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.7801 - categorical_accuracy: 0.6885 - val_loss: 1.0338 - val_categorical_accuracy: 0.7381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e4e6c8d60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Embedding(20000, 100, input_length=80))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.4, epochs=20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3dd053e320a978d07f799f814a450c1fe80d369147c17cd11516492c119f2e3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tarproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
